{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ce28add",
   "metadata": {},
   "source": [
    "# Midterm Project - Miranda Khoury (mrk6xcb)\n",
    "\n",
    "## Overview\n",
    "\n",
    "For this project, I use the Sakila sample database. The fact table, **fact_rentals**, contains information about movie rentals at the fictional Sakila movie rental store. **Fact_rentals** combines information from Sakila's **rental**, **inventory**, and **payment** tables to streamline the dataset and reduce the number of dimension tables in the final database. My designed database also includes four dimension tables, **dim_customers**, **dim_films**, **dim_staff**, and **dim_time**.\n",
    "\n",
    "## Prerequisites\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58556b01",
   "metadata": {},
   "source": [
    "#### Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b4f394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy\n",
    "import mysql.connector\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import json\n",
    "import datetime\n",
    "import pymongo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ce681e",
   "metadata": {},
   "source": [
    "#### Declare & Assign Connection Variables for the MySQL Server, MongoDB Server, & Databases with which You'll be Working "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2dcda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "host_name = \"localhost\"\n",
    "host_ip = \"127.0.0.1\"\n",
    "port = \"3306\"\n",
    "\n",
    "# my credentials to log into my MySQL Workbench\n",
    "mysql_uid = \"root\"\n",
    "mysql_pwd = \"huntersMark6%\"\n",
    "\n",
    "# my credentials to log into my personal MongoDB cluster\n",
    "atlas_cluster_name = \"testCluster\"\n",
    "atlas_user_name = \"main_user\"\n",
    "atlas_password = \"butterfly\"\n",
    "\n",
    "# connection string to connect to my MongoDB cluster\n",
    "my_conn_str = \"mongodb+srv://main_user:butterfly@testcluster.gymn7h2.mongodb.net/?retryWrites=true&w=majority\"\n",
    "\n",
    "mysql_src_db = \"northwind\"\n",
    "mongo_src_db = \"sakila_mongo\"\n",
    "dst_db = \"sakila_dw_final\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cb24cd",
   "metadata": {},
   "source": [
    "#### Define Functions for Getting Data From and Setting Data Into Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c100bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sql_dataframe(user_id, pwd, db_name, sql_query):\n",
    "    '''Create a connection to the MySQL database'''\n",
    "    conn_str = f\"mysql+pymysql://{user_id}:{pwd}@localhost/{db_name}\"\n",
    "    sqlEngine = create_engine(conn_str, pool_recycle=3600)\n",
    "    \n",
    "    '''Invoke the pd.read_sql() function to query the database, and fill a Pandas DataFrame.'''\n",
    "    conn = sqlEngine.connect()\n",
    "    dframe = pd.read_sql(sql_query, conn);\n",
    "    conn.close()\n",
    "    \n",
    "    return dframe\n",
    "\n",
    "\n",
    "def get_mongo_dataframe(connect_str, db_name, collection, query):\n",
    "    '''Create a connection to MongoDB'''\n",
    "    client = pymongo.MongoClient(connect_str)\n",
    "    \n",
    "    '''Query MongoDB, and fill a python list with documents to create a DataFrame'''\n",
    "    db = client[db_name]\n",
    "    dframe = pd.DataFrame(list(db[collection].find(query)))\n",
    "    dframe.drop(['_id'], axis=1, inplace=True)\n",
    "    client.close()\n",
    "    return dframe\n",
    "\n",
    "\n",
    "def set_dataframe(user_id, pwd, db_name, df, table_name, pk_column, db_operation):\n",
    "    '''Create a connection to the MySQL database'''\n",
    "    conn_str = f\"mysql+pymysql://{user_id}:{pwd}@localhost/{db_name}\"\n",
    "    sqlEngine = create_engine(conn_str, pool_recycle=3600)\n",
    "    connection = sqlEngine.connect()\n",
    "    \n",
    "    '''Invoke the Pandas DataFrame .to_sql( ) function to either create, or append to, a table'''\n",
    "    if db_operation == \"insert\":\n",
    "        df.to_sql(table_name, con=connection, index=False, if_exists='replace')\n",
    "        sqlEngine.execute(f\"ALTER TABLE {table_name} ADD PRIMARY KEY ({pk_column});\")\n",
    "            \n",
    "    elif db_operation == \"update\":\n",
    "        df.to_sql(table_name, con=connection, index=False, if_exists='append')\n",
    "    \n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f94d69",
   "metadata": {},
   "source": [
    "#### Create the New Data Warehouse database, and to Use it, Switch the Connection Context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6c2ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_str = f\"mysql+pymysql://{mysql_uid}:{mysql_pwd}@{host_name}\"\n",
    "sqlEngine = create_engine(conn_str, pool_recycle=3600)\n",
    "\n",
    "sqlEngine.execute(f\"DROP DATABASE IF EXISTS `{dst_db}`;\")\n",
    "sqlEngine.execute(f\"CREATE DATABASE `{dst_db}`;\")\n",
    "sqlEngine.execute(f\"USE {dst_db};\")\n",
    "\n",
    "#sqlEngine.connect().close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39281399",
   "metadata": {},
   "source": [
    "## Sourcing Data from A SQL Server (MySQL)\n",
    "\n",
    "First, I read in some of the data I'll be working with from MySQL: the **rental**, **staff**, and **customer** tables from the Sakila database loaded on my MySQL server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ba162e",
   "metadata": {},
   "source": [
    "#### Extract Data from the Source Database Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e34641",
   "metadata": {},
   "outputs": [],
   "source": [
    "sakila_customers = \"SELECT * FROM northwind.employees;\"#\"SELECT * FROM sakila.customer;\"\n",
    "df_customers = get_sql_dataframe(mysql_uid, mysql_pwd, mysql_src_db, sakila_customers)\n",
    "df_customers.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47b4d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sakila_rentals = \"SELECT * FROM sakila.rental;\"\n",
    "df_rentals = get_sql_dataframe(mysql_uid, mysql_pwd, mysql_src_db, sakila_rentals)\n",
    "df_rentals.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e17148",
   "metadata": {},
   "outputs": [],
   "source": [
    "sakila_staff = \"SELECT * FROM sakila.staff;\"\n",
    "df_staff = get_sql_dataframe(mysql_uid, mysql_pwd, mysql_src_db, sakila_staff)\n",
    "df_staff.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae09e71",
   "metadata": {},
   "source": [
    "## Sourcing Data from A NoSQL Server (MongoDB)\n",
    "\n",
    "Then, I read in some more data I'll be working with, the **inventory** and **film** tables from Sakila. First, I load the JSON versions of these tables into MongoDB. Then, we can pretend this data was only available from MongoDB to start with, and I will read it back into this Jupyter Notebook to conduct transformations on.\n",
    "\n",
    "#### Populate MongoDB with Source Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23982e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient(my_conn_str)\n",
    "db = client[mongo_src_db]\n",
    "\n",
    "# note that the data are all in the same folder that this JN file is in, so the default data path \n",
    "# is the data path needed to access the files\n",
    "data_dir = os.getcwd()\n",
    "\n",
    "json_files = {\"inventory\" : 'inventory.json',\n",
    "              \"film\" : \"sakila_film.json\"\n",
    "            }\n",
    "\n",
    "for file in json_files:\n",
    "    db.drop_collection(file)\n",
    "    json_file = os.path.join(data_dir, json_files[file])\n",
    "    with open(json_file, 'r') as openfile:\n",
    "        json_object = json.load(openfile)\n",
    "        file = db[file]\n",
    "        result = file.insert_many(json_object)\n",
    "        print(f\"{file} was successfully loaded.\")\n",
    "\n",
    "        \n",
    "client.close()        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675e4e57",
   "metadata": {},
   "source": [
    "#### Extract Data from the Source MongoDB Collections Into DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36413470",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {}\n",
    "collection = \"inventory\"\n",
    "\n",
    "df_inventory = get_mongo_dataframe(my_conn_str, mongo_src_db, collection, query)\n",
    "df_inventory.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f1a99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {}\n",
    "collection = \"film\"\n",
    "\n",
    "df_films = get_mongo_dataframe(my_conn_str, mongo_src_db, collection, query)\n",
    "df_films.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6818b6",
   "metadata": {},
   "source": [
    "## Sourcing Data from A File System\n",
    "\n",
    "For the last step of the Extract phase, I read in the final pieces of data from a local file system. The file I'll be reading in is in CSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd252f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that the data are all in the same folder that this JN file is in, so the default data path \n",
    "# is the data path needed to access the files\n",
    "data_dir = os.getcwd()\n",
    "data_file = os.path.join(data_dir, 'sakila_payment.csv')\n",
    "\n",
    "df_payments = pd.read_csv(data_file, header=0, index_col=0)\n",
    "df_payments.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe945cdd",
   "metadata": {},
   "source": [
    "## Transformation of Data Using Pandas Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688bd2ea",
   "metadata": {},
   "source": [
    "Now that all the raw tables -- **rental**, **film**, **inventory**, **customer**, and **payment** -- have been read in from their various sources, I can perform necessary transformations on the tables and combine some to form the fact table.\n",
    "\n",
    "First, I'll perform some transformations on the dimension tables.\n",
    "\n",
    "#### Transform the Customers Dimension Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a08898",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['store_id','address_id','active','create_date','last_update']\n",
    "df_customers.drop(drop_cols, axis=1, inplace=True)\n",
    "df_customers.rename(columns={\"customer_id\":\"customer_key\"}, inplace=True)\n",
    "\n",
    "df_customers.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d03ceb",
   "metadata": {},
   "source": [
    "#### Transform the Films Dimension Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83365183",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['language_id','original_language_id']\n",
    "df_films.drop(drop_cols, axis=1, inplace=True)\n",
    "df_films.rename(columns={\"film_id\":\"film_key\"}, inplace=True)\n",
    "\n",
    "df_films.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8280473",
   "metadata": {},
   "source": [
    "#### Transform the Staff Dimension Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d273ef07",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['address_id','picture','password', 'last_update', 'active', 'store_id']\n",
    "df_staff.drop(drop_cols, axis=1, inplace=True)\n",
    "df_staff.rename(columns={\"staff_id\":\"staff_key\"}, inplace=True)\n",
    "\n",
    "df_staff.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd18f8b",
   "metadata": {},
   "source": [
    "Then, I prep the inventory, payment, and rental tables to be ready to be joined to form the fact rental table.\n",
    "#### Pre-transforming the Tables that Will Become Fact Rentals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2ce1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['last_update']\n",
    "df_rentals.drop(drop_cols, axis=1, inplace=True)\n",
    "df_rentals.rename(columns={\"rental_id\":\"rental_key\", \"inventory_id\":\"inventory_key\", \"customer_id\":\"customer_key\",\"staff_id\":\"staff_key\"}, inplace=True)\n",
    "\n",
    "df_rentals.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bf9aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['last_update', \"store_id\"]\n",
    "df_inventory.drop(drop_cols, axis=1, inplace=True)\n",
    "df_inventory.rename(columns={\"inventory_id\":\"inventory_key\"}, inplace=True)\n",
    "\n",
    "df_inventory.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873eb9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['last_update', \"customer_id\", \"staff_id\"]\n",
    "df_payments.drop(drop_cols, axis=1, inplace=True)\n",
    "df_payments.rename(columns={\"rental_id\":\"rental_key\", \"payment_id\":\"payment_key\"}, inplace=True)\n",
    "\n",
    "df_payments.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96336ac2",
   "metadata": {},
   "source": [
    "Now we can merge the tables together.\n",
    "#### Merging Rentals, Payment, and Inventory into Fact Rentals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14f98db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, join payments to rentals on the rental key.\n",
    "# There is a one-to-one relationship between each rental and payment. i.e. there is a unique payment for each rental\n",
    "# and a unique rental for each payment. We can therefore use an inner join.\n",
    "df_fact_rentals1 = pd.merge(df_rentals, df_payments, on='rental_key', how='left')\n",
    "df_fact_rentals1.rename(columns={\"amount\":\"payment_amount\"}, inplace=True)\n",
    "\n",
    "df_fact_rentals1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feac327a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact_rentals1.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267e161c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# then, join inventory to rentals on the inventory key. Inventory ids are repeated \n",
    "# between rentals i.e. rental 3 and 13 both could correspond to inventory id 26. left join.\n",
    "# then, drop inventory key\n",
    "df_fact_rentals = pd.merge(df_fact_rentals1, df_inventory, on='inventory_key', how='left')\n",
    "df_fact_rentals.drop(['inventory_key'], axis=1, inplace=True)\n",
    "df_fact_rentals.rename(columns={\"film_id\":\"film_key\"}, inplace=True)\n",
    "\n",
    "df_fact_rentals.head(10).sort_values(by=['rental_key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a22991",
   "metadata": {},
   "source": [
    "## Making A Date Dimension Table Using SQL\n",
    "Now that the raw data tables are all read in and have been transformed most of the way, I'll make a data table from scratch -- one for the new date dimension I'm going to add to the data warehouse. I'll do this using SQL commands. Then, I'll integrate it into the data warehouse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b90908",
   "metadata": {},
   "source": [
    "##### 2.2.5. Get the Data from the Date Dimension Table.\n",
    "First, fetch the Surrogate Primary Key (date_key) and the Business Key (full_date) from the Date Dimension table using the **get_dataframe()** function. Also, be certain to cast the **full_date** column to the **datetime64** data type using the **.astype()** function that is native to Pandas DataFrame columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32c604a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_dim_date = \"SELECT date_key, full_date FROM dim_date;\"\n",
    "df_dim_date = get_sql_dataframe(mysql_uid, mysql_pwd, dst_db, sql_dim_date)\n",
    "df_dim_date.full_date = df_dim_date.full_date.astype('datetime64')\n",
    "df_dim_date.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0c22be",
   "metadata": {},
   "source": [
    "##### 2.2.6. Lookup the DateKeys from the Date Dimension Table.\n",
    "Next, for each date typed column in the fact table, lookup the corresponding Surrogate Primary Key column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dff7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lookup the Surrogate Primary Key (date_key) that Corresponds to the \"rental_date\" Column.\n",
    "df_dim_rental_date = df_dim_date.rename(columns={\"date_key\" : \"order_date_key\", \"full_date\" : \"order_date\"})\n",
    "df_fact_orders = pd.merge(df_fact_orders, df_dim_rental_date, on='order_date', how='inner')\n",
    "df_fact_orders.drop(['order_date'], axis=1, inplace=True) \n",
    "df_fact_orders.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153bc86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lookup the Surrogate Primary Key (date_key) that Corresponds to the \"return_date\" Column.\n",
    "df_dim_paid_date = df_dim_date.rename(columns={\"date_key\" : \"paid_date_key\", \"full_date\" : \"paid_date\"})\n",
    "df_fact_orders = pd.merge(df_fact_orders, df_dim_paid_date, on='paid_date', how='inner')\n",
    "df_fact_orders.drop(['paid_date'], axis=1, inplace=True)\n",
    "df_fact_orders.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f354e39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lookup the Surrogate Primary Key (date_key) that Corresponds to the \"shipped_date\" Column.\n",
    "df_dim_shipped_date = df_dim_date.rename(columns={\"date_key\" : \"shipped_date_key\", \"full_date\" : \"shipped_date\"})\n",
    "df_fact_orders = pd.merge(df_fact_orders, df_dim_shipped_date, on='shipped_date', how='inner')\n",
    "df_fact_orders.drop(['shipped_date'], axis=1, inplace=True)\n",
    "df_fact_orders.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f774be",
   "metadata": {},
   "source": [
    "## Loading the Tables Into the Destination Database\n",
    "\n",
    "Then, I'll load the finalized tables into my destination database, and the data warehouse is done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab92506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in the dimension tables\n",
    "db_operation = \"insert\"\n",
    "\n",
    "tables = [('dim_customers', df_customers, 'customer_key'),\n",
    "          ('dim_staff', df_staff, 'staff_key'),\n",
    "          ('dim_films', df_products, 'film_key')]\n",
    "\n",
    "for table_name, dataframe, primary_key in tables:\n",
    "    set_dataframe(mysql_uid, mysql_pwd, dst_db, dataframe, table_name, primary_key, db_operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14101432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in the fact table\n",
    "\n",
    "db_operation = \"insert\"\n",
    "\n",
    "table_name = 'fact_rentals'\n",
    "dataframe = df_fact_rentals\n",
    "primary_key = 'rental_key'\n",
    "\n",
    "set_dataframe(mysql_uid, mysql_pwd, dst_db, dataframe, table_name, primary_key, db_operation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd464948",
   "metadata": {},
   "source": [
    "## Querying the Finalized Data Warehouse\n",
    "Finally, I'll write some queries to prove that my data warehouse was implemented successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ccf41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = '''\n",
    "SELECT * FROM fact_rentals;\n",
    "'''\n",
    "\n",
    "df_test = get_sql_dataframe(mysql_uid, mysql_pwd, dst_db, sql_query)\n",
    "df_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828c0e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'store_key': [1, 2], 'address_key': [3, 4]}\n",
    "df_test = pd.DataFrame(data=d)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8a1755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in the fact table\n",
    "\n",
    "db_operation = \"insert\"\n",
    "\n",
    "table_name = 'test_dim'\n",
    "dataframe = df_test\n",
    "primary_key = 'store_key'\n",
    "\n",
    "set_dataframe(mysql_uid, mysql_pwd, dst_db, dataframe, table_name, primary_key, db_operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49d13f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = '''\n",
    "SELECT * FROM test_dim;\n",
    "'''\n",
    "\n",
    "df = get_sql_dataframe(mysql_uid, mysql_pwd, dst_db, sql_query)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
